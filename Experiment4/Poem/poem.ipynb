{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r18PS0plVdC3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "text = \"\"\n",
        "with open(\"/content/poems-100.csv\", \"r\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        text += \" \".join(row) + \" \"                          # Combine All Lines into a Single Text"
      ],
      "metadata": {
        "id": "UM7Lkza3WJSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split()"
      ],
      "metadata": {
        "id": "gCt5uOwGX5_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary to Map Words to Indices\n",
        "word_to_idx = {}\n",
        "idx_to_word = {}\n",
        "vocab_size = 0\n",
        "\n",
        "for word in tokens:\n",
        "    if word not in word_to_idx:\n",
        "        word_to_idx[word] = vocab_size\n",
        "        idx_to_word[vocab_size] = word\n",
        "        vocab_size += 1"
      ],
      "metadata": {
        "id": "yAS_3ko0X76q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_indices = [word_to_idx[word] for word in tokens]"
      ],
      "metadata": {
        "id": "-MaSVvyfYE7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Sequences and Targets\n",
        "seq_length = 10\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(token_indices) - seq_length):\n",
        "    seq = token_indices[i:i + seq_length]\n",
        "    target = token_indices[i + seq_length]\n",
        "    sequences.append(seq)\n",
        "    targets.append(target)"
      ],
      "metadata": {
        "id": "mytPTSFlYJek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = torch.tensor(sequences, dtype = torch.long)\n",
        "targets = torch.tensor(targets, dtype = torch.long)"
      ],
      "metadata": {
        "id": "Ma5Dn3snYRxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define One-Hot Encoding for RNN Model\n",
        "class OneHotRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, output_dim):\n",
        "        super(OneHotRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_dim, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.rnn(x)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "-MROOSPuYhKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM Model with Embedding Layer\n",
        "class PoemLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(PoemLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "K1_Ol4XoYloI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = vocab_size"
      ],
      "metadata": {
        "id": "q4YzYr2hYp5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_model = OneHotRNN(vocab_size, hidden_dim, output_dim)\n",
        "embedding_model = PoemLSTM(vocab_size, embed_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "Wfou-L7JYuXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "onehot_optimizer = optim.Adam(onehot_model.parameters(), lr = 0.001)\n",
        "embedding_optimizer = optim.Adam(embedding_model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "dLfNSOTLYyaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_losses, embedding_losses = [], []"
      ],
      "metadata": {
        "id": "qbqY1VNSY1bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function with Tracking\n",
        "def train_model(model, optimizer, name):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(100):\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(sequences), 32):\n",
        "            batch_seq = sequences[i:i + 32]\n",
        "            batch_target = targets[i:i + 32]\n",
        "\n",
        "            # One-Hot Encoding for OneHotRNN\n",
        "            if name == \"OneHotRNN\":\n",
        "                batch_seq = F.one_hot(batch_seq, num_classes = vocab_size).float()\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = model(batch_seq)\n",
        "            loss = criterion(outputs, batch_target)\n",
        "\n",
        "            # Backward Pass and Optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / (len(sequences) // 32)\n",
        "        if name == \"OneHotRNN\":\n",
        "            onehot_losses.append(avg_loss)\n",
        "        else:\n",
        "            embedding_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"{name} Epoch [{epoch+1}/100], Avg Loss: {avg_loss:.4f}\")\n",
        "    print(f\"{name} Training Time: {time.time() - start_time:.2f}s\\n\")"
      ],
      "metadata": {
        "id": "z7bvOiv-Y6Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Poem Generation Function\n",
        "def generate_poem(model, seed_text, num_words = 50, model_type = \"EmbeddingLSTM\"):\n",
        "    model.eval()\n",
        "    words = seed_text.split()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_words):\n",
        "            seq = [word_to_idx.get(word, 0) for word in words[-seq_length:]]\n",
        "            seq = torch.tensor(seq, dtype = torch.long).unsqueeze(0)\n",
        "\n",
        "            if model_type == \"OneHotRNN\":\n",
        "                seq = F.one_hot(seq, num_classes = vocab_size).float()\n",
        "\n",
        "            output = model(seq)\n",
        "            probabilities = F.softmax(output, dim = 1)\n",
        "            predicted_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            words.append(idx_to_word[predicted_idx])\n",
        "\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "fxPN1xu8Y-Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(onehot_model, onehot_optimizer, \"OneHotRNN\")\n",
        "train_model(embedding_model, embedding_optimizer, \"EmbeddingLSTM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rthoFSpZB5D",
        "outputId": "c396a1d4-6a9e-4503-a13a-af9685247578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OneHotRNN Epoch [1/100], Avg Loss: 7.5746\n",
            "OneHotRNN Epoch [2/100], Avg Loss: 6.6823\n",
            "OneHotRNN Epoch [3/100], Avg Loss: 6.3461\n",
            "OneHotRNN Epoch [4/100], Avg Loss: 6.1044\n",
            "OneHotRNN Epoch [5/100], Avg Loss: 5.9289\n",
            "OneHotRNN Epoch [6/100], Avg Loss: 5.7537\n",
            "OneHotRNN Epoch [7/100], Avg Loss: 5.6086\n",
            "OneHotRNN Epoch [8/100], Avg Loss: 5.4260\n",
            "OneHotRNN Epoch [9/100], Avg Loss: 5.2595\n",
            "OneHotRNN Epoch [10/100], Avg Loss: 5.0068\n",
            "OneHotRNN Epoch [11/100], Avg Loss: 4.7621\n",
            "OneHotRNN Epoch [12/100], Avg Loss: 4.5101\n",
            "OneHotRNN Epoch [13/100], Avg Loss: 4.2144\n",
            "OneHotRNN Epoch [14/100], Avg Loss: 4.0402\n",
            "OneHotRNN Epoch [15/100], Avg Loss: 3.8027\n",
            "OneHotRNN Epoch [16/100], Avg Loss: 3.4376\n",
            "OneHotRNN Epoch [17/100], Avg Loss: 3.0888\n",
            "OneHotRNN Epoch [18/100], Avg Loss: 2.8160\n",
            "OneHotRNN Epoch [19/100], Avg Loss: 2.6358\n",
            "OneHotRNN Epoch [20/100], Avg Loss: 2.4532\n",
            "OneHotRNN Epoch [21/100], Avg Loss: 2.2614\n",
            "OneHotRNN Epoch [22/100], Avg Loss: 2.0336\n",
            "OneHotRNN Epoch [23/100], Avg Loss: 1.7887\n",
            "OneHotRNN Epoch [24/100], Avg Loss: 1.5654\n",
            "OneHotRNN Epoch [25/100], Avg Loss: 1.3685\n",
            "OneHotRNN Epoch [26/100], Avg Loss: 1.2134\n",
            "OneHotRNN Epoch [27/100], Avg Loss: 1.0714\n",
            "OneHotRNN Epoch [28/100], Avg Loss: 0.9225\n",
            "OneHotRNN Epoch [29/100], Avg Loss: 0.7909\n",
            "OneHotRNN Epoch [30/100], Avg Loss: 0.6725\n",
            "OneHotRNN Epoch [31/100], Avg Loss: 0.5677\n",
            "OneHotRNN Epoch [32/100], Avg Loss: 0.4796\n",
            "OneHotRNN Epoch [33/100], Avg Loss: 0.4033\n",
            "OneHotRNN Epoch [34/100], Avg Loss: 0.3466\n",
            "OneHotRNN Epoch [35/100], Avg Loss: 0.3122\n",
            "OneHotRNN Epoch [36/100], Avg Loss: 0.2789\n",
            "OneHotRNN Epoch [37/100], Avg Loss: 0.2468\n",
            "OneHotRNN Epoch [38/100], Avg Loss: 0.2168\n",
            "OneHotRNN Epoch [39/100], Avg Loss: 0.1822\n",
            "OneHotRNN Epoch [40/100], Avg Loss: 0.1556\n",
            "OneHotRNN Epoch [41/100], Avg Loss: 0.1326\n",
            "OneHotRNN Epoch [42/100], Avg Loss: 0.1109\n",
            "OneHotRNN Epoch [43/100], Avg Loss: 0.1000\n",
            "OneHotRNN Epoch [44/100], Avg Loss: 0.0893\n",
            "OneHotRNN Epoch [45/100], Avg Loss: 0.0811\n",
            "OneHotRNN Epoch [46/100], Avg Loss: 0.0735\n",
            "OneHotRNN Epoch [47/100], Avg Loss: 0.0645\n",
            "OneHotRNN Epoch [48/100], Avg Loss: 0.0590\n",
            "OneHotRNN Epoch [49/100], Avg Loss: 0.0605\n",
            "OneHotRNN Epoch [50/100], Avg Loss: 0.0646\n",
            "OneHotRNN Epoch [51/100], Avg Loss: 0.0548\n",
            "OneHotRNN Epoch [52/100], Avg Loss: 0.0514\n",
            "OneHotRNN Epoch [53/100], Avg Loss: 0.0434\n",
            "OneHotRNN Epoch [54/100], Avg Loss: 0.0429\n",
            "OneHotRNN Epoch [55/100], Avg Loss: 0.0467\n",
            "OneHotRNN Epoch [56/100], Avg Loss: 0.0412\n",
            "OneHotRNN Epoch [57/100], Avg Loss: 0.0379\n",
            "OneHotRNN Epoch [58/100], Avg Loss: 0.0348\n",
            "OneHotRNN Epoch [59/100], Avg Loss: 0.0400\n",
            "OneHotRNN Epoch [60/100], Avg Loss: 0.0310\n",
            "OneHotRNN Epoch [61/100], Avg Loss: 0.0328\n",
            "OneHotRNN Epoch [62/100], Avg Loss: 0.0287\n",
            "OneHotRNN Epoch [63/100], Avg Loss: 0.0248\n",
            "OneHotRNN Epoch [64/100], Avg Loss: 0.0228\n",
            "OneHotRNN Epoch [65/100], Avg Loss: 0.0325\n",
            "OneHotRNN Epoch [66/100], Avg Loss: 0.0314\n",
            "OneHotRNN Epoch [67/100], Avg Loss: 0.0282\n",
            "OneHotRNN Epoch [68/100], Avg Loss: 0.0237\n",
            "OneHotRNN Epoch [69/100], Avg Loss: 0.0258\n",
            "OneHotRNN Epoch [70/100], Avg Loss: 0.0230\n",
            "OneHotRNN Epoch [71/100], Avg Loss: 0.0266\n",
            "OneHotRNN Epoch [72/100], Avg Loss: 0.0265\n",
            "OneHotRNN Epoch [73/100], Avg Loss: 0.0182\n",
            "OneHotRNN Epoch [74/100], Avg Loss: 0.0164\n",
            "OneHotRNN Epoch [75/100], Avg Loss: 0.0239\n",
            "OneHotRNN Epoch [76/100], Avg Loss: 0.0243\n",
            "OneHotRNN Epoch [77/100], Avg Loss: 0.0184\n",
            "OneHotRNN Epoch [78/100], Avg Loss: 0.0134\n",
            "OneHotRNN Epoch [79/100], Avg Loss: 0.0146\n",
            "OneHotRNN Epoch [80/100], Avg Loss: 0.0340\n",
            "OneHotRNN Epoch [81/100], Avg Loss: 0.0223\n",
            "OneHotRNN Epoch [82/100], Avg Loss: 0.0131\n",
            "OneHotRNN Epoch [83/100], Avg Loss: 0.0121\n",
            "OneHotRNN Epoch [84/100], Avg Loss: 0.0174\n",
            "OneHotRNN Epoch [85/100], Avg Loss: 0.0185\n",
            "OneHotRNN Epoch [86/100], Avg Loss: 0.0128\n",
            "OneHotRNN Epoch [87/100], Avg Loss: 0.0139\n",
            "OneHotRNN Epoch [88/100], Avg Loss: 0.0152\n",
            "OneHotRNN Epoch [89/100], Avg Loss: 0.0130\n",
            "OneHotRNN Epoch [90/100], Avg Loss: 0.0180\n",
            "OneHotRNN Epoch [91/100], Avg Loss: 0.0178\n",
            "OneHotRNN Epoch [92/100], Avg Loss: 0.0189\n",
            "OneHotRNN Epoch [93/100], Avg Loss: 0.0139\n",
            "OneHotRNN Epoch [94/100], Avg Loss: 0.0104\n",
            "OneHotRNN Epoch [95/100], Avg Loss: 0.0085\n",
            "OneHotRNN Epoch [96/100], Avg Loss: 0.0102\n",
            "OneHotRNN Epoch [97/100], Avg Loss: 0.0221\n",
            "OneHotRNN Epoch [98/100], Avg Loss: 0.0120\n",
            "OneHotRNN Epoch [99/100], Avg Loss: 0.0080\n",
            "OneHotRNN Epoch [100/100], Avg Loss: 0.0098\n",
            "OneHotRNN Training Time: 4704.36s\n",
            "\n",
            "EmbeddingLSTM Epoch [1/100], Avg Loss: 7.5619\n",
            "EmbeddingLSTM Epoch [2/100], Avg Loss: 6.5299\n",
            "EmbeddingLSTM Epoch [3/100], Avg Loss: 6.0113\n",
            "EmbeddingLSTM Epoch [4/100], Avg Loss: 5.4903\n",
            "EmbeddingLSTM Epoch [5/100], Avg Loss: 5.0294\n",
            "EmbeddingLSTM Epoch [6/100], Avg Loss: 4.6126\n",
            "EmbeddingLSTM Epoch [7/100], Avg Loss: 4.1834\n",
            "EmbeddingLSTM Epoch [8/100], Avg Loss: 3.7606\n",
            "EmbeddingLSTM Epoch [9/100], Avg Loss: 3.3454\n",
            "EmbeddingLSTM Epoch [10/100], Avg Loss: 2.9612\n",
            "EmbeddingLSTM Epoch [11/100], Avg Loss: 2.5165\n",
            "EmbeddingLSTM Epoch [12/100], Avg Loss: 2.1295\n",
            "EmbeddingLSTM Epoch [13/100], Avg Loss: 1.8082\n",
            "EmbeddingLSTM Epoch [14/100], Avg Loss: 1.5478\n",
            "EmbeddingLSTM Epoch [15/100], Avg Loss: 1.3345\n",
            "EmbeddingLSTM Epoch [16/100], Avg Loss: 1.1370\n",
            "EmbeddingLSTM Epoch [17/100], Avg Loss: 0.9751\n",
            "EmbeddingLSTM Epoch [18/100], Avg Loss: 0.8435\n",
            "EmbeddingLSTM Epoch [19/100], Avg Loss: 0.7326\n",
            "EmbeddingLSTM Epoch [20/100], Avg Loss: 0.6262\n",
            "EmbeddingLSTM Epoch [21/100], Avg Loss: 0.5527\n",
            "EmbeddingLSTM Epoch [22/100], Avg Loss: 0.4969\n",
            "EmbeddingLSTM Epoch [23/100], Avg Loss: 0.4541\n",
            "EmbeddingLSTM Epoch [24/100], Avg Loss: 0.4105\n",
            "EmbeddingLSTM Epoch [25/100], Avg Loss: 0.3869\n",
            "EmbeddingLSTM Epoch [26/100], Avg Loss: 0.3482\n",
            "EmbeddingLSTM Epoch [27/100], Avg Loss: 0.3473\n",
            "EmbeddingLSTM Epoch [28/100], Avg Loss: 0.3570\n",
            "EmbeddingLSTM Epoch [29/100], Avg Loss: 0.3606\n",
            "EmbeddingLSTM Epoch [30/100], Avg Loss: 0.3787\n",
            "EmbeddingLSTM Epoch [31/100], Avg Loss: 0.5031\n",
            "EmbeddingLSTM Epoch [32/100], Avg Loss: 0.7395\n",
            "EmbeddingLSTM Epoch [33/100], Avg Loss: 0.6676\n",
            "EmbeddingLSTM Epoch [34/100], Avg Loss: 0.3585\n",
            "EmbeddingLSTM Epoch [35/100], Avg Loss: 0.1736\n",
            "EmbeddingLSTM Epoch [36/100], Avg Loss: 0.0985\n",
            "EmbeddingLSTM Epoch [37/100], Avg Loss: 0.0617\n",
            "EmbeddingLSTM Epoch [38/100], Avg Loss: 0.0432\n",
            "EmbeddingLSTM Epoch [39/100], Avg Loss: 0.0298\n",
            "EmbeddingLSTM Epoch [40/100], Avg Loss: 0.0207\n",
            "EmbeddingLSTM Epoch [41/100], Avg Loss: 0.0418\n",
            "EmbeddingLSTM Epoch [42/100], Avg Loss: 0.2842\n",
            "EmbeddingLSTM Epoch [43/100], Avg Loss: 0.1186\n",
            "EmbeddingLSTM Epoch [44/100], Avg Loss: 0.0391\n",
            "EmbeddingLSTM Epoch [45/100], Avg Loss: 0.0179\n",
            "EmbeddingLSTM Epoch [46/100], Avg Loss: 0.0106\n",
            "EmbeddingLSTM Epoch [47/100], Avg Loss: 0.0072\n",
            "EmbeddingLSTM Epoch [48/100], Avg Loss: 0.0053\n",
            "EmbeddingLSTM Epoch [49/100], Avg Loss: 0.0040\n",
            "EmbeddingLSTM Epoch [50/100], Avg Loss: 0.0030\n",
            "EmbeddingLSTM Epoch [51/100], Avg Loss: 0.0023\n",
            "EmbeddingLSTM Epoch [52/100], Avg Loss: 0.0017\n",
            "EmbeddingLSTM Epoch [53/100], Avg Loss: 0.0013\n",
            "EmbeddingLSTM Epoch [54/100], Avg Loss: 0.1732\n",
            "EmbeddingLSTM Epoch [55/100], Avg Loss: 0.2968\n",
            "EmbeddingLSTM Epoch [56/100], Avg Loss: 0.0669\n",
            "EmbeddingLSTM Epoch [57/100], Avg Loss: 0.0209\n",
            "EmbeddingLSTM Epoch [58/100], Avg Loss: 0.0100\n",
            "EmbeddingLSTM Epoch [59/100], Avg Loss: 0.0063\n",
            "EmbeddingLSTM Epoch [60/100], Avg Loss: 0.0053\n",
            "EmbeddingLSTM Epoch [61/100], Avg Loss: 0.0043\n",
            "EmbeddingLSTM Epoch [62/100], Avg Loss: 0.0040\n",
            "EmbeddingLSTM Epoch [63/100], Avg Loss: 0.1033\n",
            "EmbeddingLSTM Epoch [64/100], Avg Loss: 0.1719\n",
            "EmbeddingLSTM Epoch [65/100], Avg Loss: 0.0499\n",
            "EmbeddingLSTM Epoch [66/100], Avg Loss: 0.0127\n",
            "EmbeddingLSTM Epoch [67/100], Avg Loss: 0.0055\n",
            "EmbeddingLSTM Epoch [68/100], Avg Loss: 0.0035\n",
            "EmbeddingLSTM Epoch [69/100], Avg Loss: 0.0026\n",
            "EmbeddingLSTM Epoch [70/100], Avg Loss: 0.0020\n",
            "EmbeddingLSTM Epoch [71/100], Avg Loss: 0.0016\n",
            "EmbeddingLSTM Epoch [72/100], Avg Loss: 0.0012\n",
            "EmbeddingLSTM Epoch [73/100], Avg Loss: 0.0009\n",
            "EmbeddingLSTM Epoch [74/100], Avg Loss: 0.0007\n",
            "EmbeddingLSTM Epoch [75/100], Avg Loss: 0.0005\n",
            "EmbeddingLSTM Epoch [76/100], Avg Loss: 0.0276\n",
            "EmbeddingLSTM Epoch [77/100], Avg Loss: 0.3207\n",
            "EmbeddingLSTM Epoch [78/100], Avg Loss: 0.0699\n",
            "EmbeddingLSTM Epoch [79/100], Avg Loss: 0.0161\n",
            "EmbeddingLSTM Epoch [80/100], Avg Loss: 0.0061\n",
            "EmbeddingLSTM Epoch [81/100], Avg Loss: 0.0037\n",
            "EmbeddingLSTM Epoch [82/100], Avg Loss: 0.0028\n",
            "EmbeddingLSTM Epoch [83/100], Avg Loss: 0.0021\n",
            "EmbeddingLSTM Epoch [84/100], Avg Loss: 0.0016\n",
            "EmbeddingLSTM Epoch [85/100], Avg Loss: 0.0013\n",
            "EmbeddingLSTM Epoch [86/100], Avg Loss: 0.0010\n",
            "EmbeddingLSTM Epoch [87/100], Avg Loss: 0.0007\n",
            "EmbeddingLSTM Epoch [88/100], Avg Loss: 0.0006\n",
            "EmbeddingLSTM Epoch [89/100], Avg Loss: 0.0004\n",
            "EmbeddingLSTM Epoch [90/100], Avg Loss: 0.0003\n",
            "EmbeddingLSTM Epoch [91/100], Avg Loss: 0.0002\n",
            "EmbeddingLSTM Epoch [92/100], Avg Loss: 0.0002\n",
            "EmbeddingLSTM Epoch [93/100], Avg Loss: 0.1274\n",
            "EmbeddingLSTM Epoch [94/100], Avg Loss: 0.2448\n",
            "EmbeddingLSTM Epoch [95/100], Avg Loss: 0.0519\n",
            "EmbeddingLSTM Epoch [96/100], Avg Loss: 0.0134\n",
            "EmbeddingLSTM Epoch [97/100], Avg Loss: 0.0048\n",
            "EmbeddingLSTM Epoch [98/100], Avg Loss: 0.0029\n",
            "EmbeddingLSTM Epoch [99/100], Avg Loss: 0.0022\n",
            "EmbeddingLSTM Epoch [100/100], Avg Loss: 0.0017\n",
            "EmbeddingLSTM Training Time: 2188.86s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"I look up to \"\n",
        "print(\"\\nGenerated Poem (OneHotRNN):\", generate_poem(onehot_model, seed_text, model_type = \"OneHotRNN\"))\n",
        "print(\"\\nGenerated Poem (EmbeddingLSTM):\", generate_poem(embedding_model, seed_text, model_type = \"EmbeddingLSTM\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4fDaG24mpgC",
        "outputId": "f488ab04-2821-43a6-b058-96a1fe3a8375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Poem (OneHotRNN): I look up to each! and knots, The stream retreats and kiss belly and strange, Shall be to winds that name in their even and I friend So It is not flag as good at me. I have had withdrawn into too, And murmurs from the cool under the roots of life hand on\n",
            "\n",
            "Generated Poem (EmbeddingLSTM): I look up to a love You see in the tongue I hear the violoncello, ('tis the young man's heart's complaint,) I hear the key'd cornet, it glides quickly in through my ears, It shakes mad-sweet pangs through my belly and breast. I hear the chorus, it is a grand opera, Ah this indeed\n"
          ]
        }
      ]
    }
  ]
}